# -*- coding: utf-8 -*-
"""whisper eng encoder decoder test .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JGfP40ubNwLmcSh7kuI2wmhjR4cmDyfJ

#Loading testing data
"""

!pip install -U openai-whisper

import whisper
import torch
from google.colab import drive
import os

model = whisper.load_model("small")  # change to "small", "medium", etc. if needed

drive.mount('/content/drive')
uploaded = "/content/drive/MyDrive/Dataset/cse498r/test 30secpitch.mp3"

audio_path = uploaded
print("Audio file:", audio_path)

#Convert Audio to Compatible Format
converted_path = "converted.wav"

!ffmpeg -i "{audio_path}" -ar 16000 -ac 1 -c:a pcm_s16le "{converted_path}"

result = model.transcribe(converted_path)
print("Transcription:\n", result["text"])

"""#ONNX Conversion"""

# Install Python packages
!pip install -U onnx onnxruntime onnxruntime-tools "optimum[onnxruntime]" soundfile --quiet

import onnx

# 1) Disable SDPA (the critical line) ───────────────────────
from whisper.model import MultiHeadAttention
MultiHeadAttention.use_sdpa = False

# ── 2. Wrapper modules ─────────────────────────────────────────────────────────
class Encoder(torch.nn.Module):
    def __init__(self, w):
        super().__init__();  self.enc = w.encoder
    def forward(self, mel):                   # ← encoder needs ONLY the Mel
        return self.enc(mel)

class Decoder(torch.nn.Module):
    def __init__(self, w):
        super().__init__();  self.dec = w.decoder
    def forward(self, tokens, enc_out):       # ← decoder uses tokens + enc_out
        return self.dec(tokens, enc_out)

enc = Encoder(model).eval()
dec = Decoder(model).eval()

# ── 3. Export ENCODER ──────────────────────────────────────────────────────────
mel_dummy = torch.randn(1, 80, 3000)           # (batch, n_mels, n_frames)
torch.onnx.export(
    enc, mel_dummy, "encoder.onnx",
    input_names  = ["mel"],
    output_names = ["enc_out"],
    dynamic_axes = {"mel": {0: "batch", 2: "frames"}},
    opset_version = 17
)

# ── 4. Export DECODER ──────────────────────────────────────────────────────────
SOT_ID = 50258                                 # start-of-transcript token
tok_dummy  = torch.tensor([[SOT_ID]])          # (batch, seq_len=1)
enc_dummy  = torch.randn(1, 1500, 768)         # (batch, frames/2, d_model)

torch.onnx.export(
    dec, (tok_dummy, enc_dummy), "decoder.onnx",
    input_names  = ["tokens", "enc_out"],
    output_names = ["logits"],
    dynamic_axes = {
        "tokens":  {0: "batch", 1: "seq"},
        "enc_out": {0: "batch", 1: "frames"}
    },
    opset_version = 17
)

# ── 5. Quick sanity check ──────────────────────────────────────────────────────
onnx.checker.check_model(onnx.load("encoder.onnx"))
onnx.checker.check_model(onnx.load("decoder.onnx"))
print("✅  Encoder & decoder exported without errors!")

"""#Check encoder and decoder"""

import soundfile as sf
import numpy as np
from whisper.audio import log_mel_spectrogram, pad_or_trim
from whisper.tokenizer import get_tokenizer
import onnxruntime as ort

# 1 Use onnx.checker (structural validity)

onnx.checker.check_model(onnx.load("encoder.onnx"))
onnx.checker.check_model(onnx.load("decoder.onnx"))
print("✔ Graphs pass ONNX structural checks")

# 2 Run a shape-only smoke test with ONNX Runtime
import onnxruntime as ort
import numpy as np

# --- create sessions ---
enc_sess = ort.InferenceSession("encoder.onnx", providers=["CPUExecutionProvider"])
dec_sess = ort.InferenceSession("decoder.onnx", providers=["CPUExecutionProvider"])

# --- dummy inputs ---
mel     = np.random.randn(1, 80, 3000).astype(np.float32)   # (B, 80, T)
enc_out = enc_sess.run(None, {"mel": mel})[0]               # (B, T/2, 768)

tokens  = np.array([[50258]], dtype=np.int64)               # (B, 1)
logits  = dec_sess.run(None, {"tokens": tokens, "enc_out": enc_out})[0]

print("encoder output →", enc_out.shape)
print("decoder logits →", logits.shape)      # (B, 1, vocab=51865)

# --- load same Whisper-small in PyTorch ---
pt_model = model

# 3 Compare numerical parity with the PyTorch model

# — read raw audio —
wav, sr = sf.read("converted.wav")
assert sr == 16000, f"Expected 16 kHz, got {sr}"

# — if stereo, to mono —
if wav.ndim > 1:
    wav = wav.mean(axis=1)

# — convert numpy → torch tensor, pad/trim to 30 s —
audio = torch.from_numpy(wav.astype(np.float32))
audio = pad_or_trim(audio)         # now audio.shape == [16000*30]

# — compute mel spectrogram of the fixed‑length audio —
mel = log_mel_spectrogram(audio)   # mel.shape == [80, 3000]

# --- PyTorch reference ---
with torch.no_grad():
    enc_out_pt = pt_model.encoder(mel.unsqueeze(0))
    sot = torch.tensor([[50258]])
    logits_pt  = pt_model.decoder(sot, enc_out_pt).cpu().numpy()

# --- ONNX inference ---
enc_out_onnx = enc_sess.run(None, {"mel": mel.unsqueeze(0).numpy()})[0]
logits_onnx  = dec_sess.run(None, {"tokens": sot.numpy(), "enc_out": enc_out_onnx})[0]

# --- compare ---
diff = np.max(np.abs(logits_pt - logits_onnx))
print(f"Max abs diff = {diff:.5f}")

# 2) Load ONNX sessions
enc_sess = ort.InferenceSession("encoder.onnx", providers=["CPUExecutionProvider"])
dec_sess = ort.InferenceSession("decoder.onnx", providers=["CPUExecutionProvider"])

def transcribe_audio(audio_path):
    # --- Read & preprocess audio to a fixed 30s mel ---
    wav, sr = sf.read(audio_path)
    assert sr == 16000, f"Expected 16 kHz, got {sr}"
    if wav.ndim > 1:
        wav = wav.mean(axis=1)
    audio = torch.from_numpy(wav.astype(np.float32))
    audio = pad_or_trim(audio)                    # → length 30 s
    mel = log_mel_spectrogram(audio)              # shape [80, 3000]
    mel_batch = mel.unsqueeze(0)                  # shape [1, 80, 3000]

    # --- 1) Base model inference (PyTorch) ---
    with torch.no_grad():
        enc_out_pt = pt_model.encoder(mel_batch)  # [1, frames/2, 768]
        tokens_pt = torch.tensor([[50258]], dtype=torch.long)  # SOT token
        logits_pt = pt_model.decoder(tokens_pt, enc_out_pt)    # [1, 1, vocab_size]
        logits_pt_np = logits_pt.cpu().numpy()

    print("🌟 Base model logits shape:", logits_pt_np.shape)
    print("First 5 Base-model logits:", logits_pt_np[0, 0])

    # --- 2) ONNX inference ---
    mel_np = mel_batch.numpy().astype(np.float32)
    enc_out_onnx = enc_sess.run(None, {"mel": mel_np})[0]
    tokens_onnx = np.array([[50258]], dtype=np.int64)
    logits_onnx = dec_sess.run(None, {"tokens": tokens_onnx, "enc_out": enc_out_onnx})[0]

    print("🤖 ONNX logits shape:", logits_onnx.shape)
    print("First 5 ONNX-model logits:", logits_onnx[0, 0])

    # 1. Grab your logits arrays from above:
    #    logits_pt_np   # shape [1, 1, vocab_size]
    #    logits_onnx    # shape [1, 1, vocab_size]

    # 2. Greedy: pick the highest-probability token at this time step
    pt_id   = int(logits_pt_np[0, 0].argmax())
    onnx_id = int(logits_onnx[0, 0].argmax())

    # 3. Load the same tokenizer your model uses
    tokenizer = get_tokenizer(
        pt_model.is_multilingual,
        language="en",        # or whichever language you used
        task="transcribe"     # or "translate"
    )

    # 4. Decode single tokens to text
    text_pt   = tokenizer.decode([pt_id])
    text_onnx = tokenizer.decode([onnx_id])

    print("Greedy PT token →", pt_id, "→", text_pt)
    print("Greedy ONNX token →", onnx_id, "→", text_onnx)

# Run on your test file
transcribe_audio("converted.wav")

# If you’re on GPU, free up memory afterwards
if torch.cuda.is_available():
    torch.cuda.empty_cache()

from whisper.tokenizer import get_tokenizer

# assuming you already have pt_model loaded:
tokenizer = get_tokenizer(
    pt_model.is_multilingual,  # False for English-only
    language="en",             # or your target language
    task="transcribe"          # or "translate"
)


SOT_ID = 50258
EOT_ID = tokenizer.eot

ids = [SOT_ID]
for _ in range(100):               # max 100 tokens
    logits = dec_sess.run(
        None,
        {"tokens": np.array([ids], dtype=np.int64),
         "enc_out": enc_out_onnx}
    )[0]                         # shape [1, len(ids), vocab]
    next_id = int(logits[0, -1].argmax())
    if next_id == EOT_ID:
        break
    ids.append(next_id)

full_text = tokenizer.decode(ids[1:])  # skip the SOT token
print("🔁 ONNX full greedy decode:", full_text)

# 1) Load models & tokenizer
enc_sess  = ort.InferenceSession("encoder.onnx", providers=["CPUExecutionProvider"])
dec_sess  = ort.InferenceSession("decoder.onnx", providers=["CPUExecutionProvider"])
tokenizer = get_tokenizer(
    pt_model.is_multilingual,
    language="en",
    task="transcribe"
)

SOT_ID = 50258
EOT_ID = tokenizer.eot

def transcribe_and_decode(audio_path, max_decode_steps=100):
    # --- 1) Read & preprocess ---
    wav, sr = sf.read(audio_path)
    assert sr == 16000, f"Need 16 kHz, got {sr}"
    if wav.ndim > 1: wav = wav.mean(axis=1)
    audio = torch.from_numpy(wav.astype(np.float32))
    audio = pad_or_trim(audio)
    mel   = log_mel_spectrogram(audio)       # [80,3000]
    mel_b = mel.unsqueeze(0)                 # [1,80,3000]

    # --- 2) PyTorch logits ---
    with torch.no_grad():
        enc_pt    = pt_model.encoder(mel_b)     # [1,1500,768]
        tokens_pt = torch.tensor([[SOT_ID]])
        logits_pt = pt_model.decoder(tokens_pt, enc_pt)  # [1,1,vocab]
    logits_pt_np = logits_pt.cpu().numpy()
    print("🌟 Base logits shape:", logits_pt_np.shape)
    print("First 5 Base logits:", logits_pt_np[0,0,:5])

    # --- 3) ONNX logits ---
    enc_onnx = enc_sess.run(None, {"mel": mel_b.numpy()})[0]
    tokens_onnx = np.array([[SOT_ID]], dtype=np.int64)
    logits_onnx = dec_sess.run(
        None,
        {"tokens": tokens_onnx, "enc_out": enc_onnx}
    )[0]
    print("🤖 ONNX logits shape:", logits_onnx.shape)
    print("First 5 ONNX logits:", logits_onnx[0,0,:5])

    # --- 4) Single‐token greedy decode ---
    pt_id   = int(logits_pt_np[0,0].argmax())
    onnx_id = int(logits_onnx[0,0].argmax())
    print("Greedy PT →", pt_id, "→", tokenizer.decode([pt_id]))
    print("Greedy ONNX →", onnx_id, "→", tokenizer.decode([onnx_id]))

    # --- 5) Full‐sequence greedy decode on ONNX ---
    ids = [SOT_ID]
    for _ in range(max_decode_steps):
        out = dec_sess.run(
            None,
            {"tokens": np.array([ids],dtype=np.int64), "enc_out": enc_onnx}
        )[0]  # [1, len(ids), vocab]
        next_id = int(out[0,-1].argmax())
        if next_id == EOT_ID:
            break
        ids.append(next_id)
    full_txt = tokenizer.decode(ids[1:])
    print("🔁 Full ONNX greedy decode:", full_txt)

# Run it:
transcribe_and_decode("converted.wav")

# Clean up GPU if used:
if torch.cuda.is_available():
    torch.cuda.empty_cache()