# -*- coding: utf-8 -*-
"""whisper eng encoder decoder test .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JGfP40ubNwLmcSh7kuI2wmhjR4cmDyfJ

#Loading testing data
"""

!pip install -q git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install -y ffmpeg

!pip install -U openai-whisper

import whisper
import torch

model = whisper.load_model("base")  # change to "small", "medium", etc. if needed

from google.colab import files
uploaded = files.upload()

import os

audio_path = list(uploaded.keys())[0]
print("Audio file:", audio_path)

#Convert Audio to Compatible Format
converted_path = "converted.wav"

!ffmpeg -i "{audio_path}" -ar 16000 -ac 1 -c:a pcm_s16le "{converted_path}"

result = model.transcribe(converted_path)
print("Transcription:\n", result["text"])

"""#ONNX Conversion"""

# Install Python packages
!pip install -U openai-whisper onnx onnxruntime onnxruntime-tools "optimum[onnxruntime]" soundfile --quiet

import whisper, torch

model = whisper.load_model("small")      # change to "tiny" / "base" if needed
print(sum(p.numel() for p in model.parameters())/1e6, "M parameters")

import onnx

# ── 1. Load Whisper-small ──────────────────────────────────────────────────────
model = whisper.load_model("small").eval()      # or "tiny" / "base"

# 1) Disable SDPA (the critical line) ───────────────────────
from whisper.model import MultiHeadAttention
MultiHeadAttention.use_sdpa = False

# ── 2. Wrapper modules ─────────────────────────────────────────────────────────
class Encoder(torch.nn.Module):
    def __init__(self, w):
        super().__init__();  self.enc = w.encoder
    def forward(self, mel):                   # ← encoder needs ONLY the Mel
        return self.enc(mel)

class Decoder(torch.nn.Module):
    def __init__(self, w):
        super().__init__();  self.dec = w.decoder
    def forward(self, tokens, enc_out):       # ← decoder uses tokens + enc_out
        return self.dec(tokens, enc_out)

enc = Encoder(model).eval()
dec = Decoder(model).eval()

# ── 3. Export ENCODER ──────────────────────────────────────────────────────────
mel_dummy = torch.randn(1, 80, 3000)           # (batch, n_mels, n_frames)
torch.onnx.export(
    enc, mel_dummy, "encoder.onnx",
    input_names  = ["mel"],
    output_names = ["enc_out"],
    dynamic_axes = {"mel": {0: "batch", 2: "frames"}},
    opset_version = 17
)

# ── 4. Export DECODER ──────────────────────────────────────────────────────────
SOT_ID = 50258                                 # start-of-transcript token
tok_dummy  = torch.tensor([[SOT_ID]])          # (batch, seq_len=1)
enc_dummy  = torch.randn(1, 1500, 768)         # (batch, frames/2, d_model)

torch.onnx.export(
    dec, (tok_dummy, enc_dummy), "decoder.onnx",
    input_names  = ["tokens", "enc_out"],
    output_names = ["logits"],
    dynamic_axes = {
        "tokens":  {0: "batch", 1: "seq"},
        "enc_out": {0: "batch", 1: "frames"}
    },
    opset_version = 17
)

# ── 5. Quick sanity check ──────────────────────────────────────────────────────
onnx.checker.check_model(onnx.load("encoder.onnx"))
onnx.checker.check_model(onnx.load("decoder.onnx"))
print("✅  Encoder & decoder exported without errors!")

"""#Check encoder and decoder"""

# 1 Use onnx.checker (structural validity)
import onnx
onnx.checker.check_model(onnx.load("encoder.onnx"))
onnx.checker.check_model(onnx.load("decoder.onnx"))
print("✔ Graphs pass ONNX structural checks")

# 2 Run a shape-only smoke test with ONNX Runtime
import onnxruntime as ort
import numpy as np

# --- create sessions ---
enc_sess = ort.InferenceSession("encoder.onnx", providers=["CPUExecutionProvider"])
dec_sess = ort.InferenceSession("decoder.onnx", providers=["CPUExecutionProvider"])

# --- dummy inputs ---
mel     = np.random.randn(1, 80, 3000).astype(np.float32)   # (B, 80, T)
enc_out = enc_sess.run(None, {"mel": mel})[0]               # (B, T/2, 768)

tokens  = np.array([[50258]], dtype=np.int64)               # (B, 1)
logits  = dec_sess.run(None, {"tokens": tokens, "enc_out": enc_out})[0]

print("encoder output →", enc_out.shape)
print("decoder logits →", logits.shape)      # (B, 1, vocab=51865)

import whisper, torch
import soundfile as sf
import numpy as np
from whisper.audio import log_mel_spectrogram, pad_or_trim

# --- load same Whisper-small in PyTorch ---
pt_model = whisper.load_model("small").eval()

# 3 Compare numerical parity with the PyTorch model

# — read raw audio —
wav, sr = sf.read("converted.wav")
assert sr == 16000, f"Expected 16 kHz, got {sr}"

# — if stereo, to mono —
if wav.ndim > 1:
    wav = wav.mean(axis=1)

# — convert numpy → torch tensor, pad/trim to 30 s —
audio = torch.from_numpy(wav.astype(np.float32))
audio = pad_or_trim(audio)         # now audio.shape == [16000*30]

# — compute mel spectrogram of the fixed‑length audio —
mel = log_mel_spectrogram(audio)   # mel.shape == [80, 3000]

# --- PyTorch reference ---
with torch.no_grad():
    enc_out_pt = pt_model.encoder(mel.unsqueeze(0))
    sot = torch.tensor([[50258]])
    logits_pt  = pt_model.decoder(sot, enc_out_pt).cpu().numpy()

# --- ONNX inference ---
enc_out_onnx = enc_sess.run(None, {"mel": mel.unsqueeze(0).numpy()})[0]
logits_onnx  = dec_sess.run(None, {"tokens": sot.numpy(), "enc_out": enc_out_onnx})[0]

# --- compare ---
import numpy as np
diff = np.max(np.abs(logits_pt - logits_onnx))
print(f"Max abs diff = {diff:.5f}")